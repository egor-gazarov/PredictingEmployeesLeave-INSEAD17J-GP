---
title: "Predicting Leave of Employees"
author: "Egor Gazarov, William Handjaja, Dmitry Kalachev, Nathan Kung (INSEAD MBA Class of July 2017); Jorge Bravo"
date: "January 31, 2017"
output: 
 html_document:
    css: Styles/default.css
    theme: paper
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---

```{r , echo=TRUE, message=FALSE, warning=FALSE}
# basic preparatory commands

# clear the working environment
rm(list = ls())

# load the dataset for the project
HR <- read.csv("./Data/HR_dataset.csv", sep=",", dec=".")

# load required packages
# install required packages if loading failed
# all packages sould be listed inside install_load()
source("./R/library.R")
install_load("ggplot2", "corrplot", "caret", "C50", "cwhmisc", "stringr", "gmodels")

# load file with function we'll use in work with classification tree
#source("./R/C5.0.graphviz.R")
```

# Introduction

## Brief description of the project

This project has been done by a group of students of INSEAD course 'Big Data & Analytics for Business' (Professor [Theos Evgeniou](https://www.insead.edu/faculty-research/faculty/theodoros-evgeniou)) in January 2017. Using techniques learned in the course we will analyze dataset describing employees of an organization and build a model to predict if employee will leave a company in near future.


## Project process and document structure

Both project work and this report are structured according to following process (based on Cross Industry Standard Process for Data Mining):

**1. Business understanding.** Identification of business question to be answered by model prepared during the project.

**2. Data undestanding.** Initial analysis of the dataset used in the project using basic descriptive statistics and visualizations.

**3. Data preparation.** Preparation of the dataset for subsequent modeling.

+ Check for missing values, exclusion of corresponding observations.
+ Check for outliers, decision on their participation in analysis.
+ Conversion of non-numerical attributes to numerical dummy variables.
+ Range normalization (if required).
+ Dimensionality reduction (if required).
+ Separation of dataset into training and test.

**4. Modelling.** Building the predictive model based on training dataset with target attribute answering the chosen business question. 

For classification trees methodology, modelling process include following steps.

+ Choice of methodology and tool.
+ First application of tool to dataset.
+ Generation of decision tree visualization, decision on simplification possibility.
+ Simplification of the model if possible.
+ Preliminary evaluation of the model using confusion matrix and test dataset. Decision to proceed (if evaluation results are satisfactory) or repeating the modelling process.

**5. Evaluation.** Check of models predictive accuracy on test dataset.

+ Generation of confusion matrix using built model and test dataset.
+ Decision on accepting the model or repeating the modelling process.

**6. Deployment.** Conclusion on project results and description of potential deployment of built model in practice. Remarks about potential ways to improve process and model of this project.


# Business Understanding

People are the most important resource for a lot of companies. And employees turnover has always been one of major HR issues. Retaining workers is becoming a balancing act between hard data research and a human touch. Itâ€™s more important than ever to complement intuition with statistical analysis (more on this available, for example on [INSEAD Knowledge](http://knowledge.insead.edu/blog/insead-blog/the-art-of-keeping-employees-from-leaving-3896).

Business problem that this project intends to solve is **prediction of employees to leave in near future using attributes known about each of employees**.

# Data Understanding

## Source of data

For this project we use dataset 'Human Resources Analytics' published by Ludovic Benistant on kaggle ([source](https://www.kaggle.com/ludobenistant/hr-analytics/)) under CC BY-SA 4.0 License. 

## Dataset size and variables

Dataset contains `r nrow(HR)` observations. Each observation represents information about one employee across `r ncol(HR)` variables. Variables are (we indicate code of variable in dataset in parentheses):

+ Employee satisfaction level *(satisfaction_level)*
+ Last evaluation *(last_evaluation)*
+ Number of projects *(number_project)*
+ Average monthly hours *(average_montly_hours)*
+ Time spent at the company *(time_spend_company)*
+ Whether they have had a work accident *(Work_accident)*
+ Whether they have had a promotion in the last 5 years *(promotion_last_5years)*
+ Department *(sales)*
+ Salary *(salary)*
+ Whether the employee has left, the target variable in this project *(left)*

Below is summary of project dataset structure.

```{r}
str(HR)
```

## Distributions of variables {.tabset}

First, let's generate summary report for all variables.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
summary(HR)
```

We will visualize distributions of variables separately for people who left the company and who stayed to be able to see differences between these groups of people on charts before doing the actual modelling.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
HR.left <- subset(HR, left == 1)
HR.stayed <- subset(HR, left == 0)
```

There are `r nrow(HR.left)` observations of employees who left the company and `r nrow(HR.stayed)` observations of employees who stayed.

### Satisfaction level

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.left, aes(HR.left$satisfaction_level)) + geom_density(kernel = "gaussian", fill = '#B8274C', alpha = 0.3) + labs(x = "Satisfaction level of employees who left") + xlim(0, 1)
```

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.stayed, aes(HR.stayed$satisfaction_level)) + geom_density(kernel = "gaussian", fill = '#006E51', alpha = 0.3) + labs(x = "Satisfaction level of employees who stayed") + xlim(0, 1)
```

### Last evaluation

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.left, aes(HR.left$last_evaluation)) + geom_density(kernel = "gaussian", fill = '#B8274C', alpha = 0.3) + labs(x = "Last evaluation of employees who left") + xlim(0, 1)
```

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.stayed, aes(HR.stayed$last_evaluation)) + geom_density(kernel = "gaussian", fill = '#006E51', alpha = 0.3) + labs(x = "Last evaluation of employees who stayed") + xlim(0, 1)
```

### Number of projects

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.left, aes(HR.left$number_project)) + geom_histogram(fill = '#B8274C', binwidth = .5) + scale_x_continuous(breaks = seq(0,max(max(HR.left$number_project),max(HR.stayed$number_project)),by = 1), limits = c(0, max(max(HR.left$number_project),max(HR.stayed$number_project))+1)) + labs(x = "Number of projects of employees who left")
```

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.stayed, aes(HR.stayed$number_project)) + geom_histogram(fill = '#006E51', binwidth = .5) + scale_x_continuous(breaks = seq(0,max(max(HR.left$number_project),max(HR.stayed$number_project)),by = 1), limits = c(0, max(max(HR.left$number_project),max(HR.stayed$number_project))+1)) + labs(x = "Number of projects of employees who stayed")
```

### Monthly hours

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.left, aes(HR.left$average_montly_hours)) + geom_density(kernel = "gaussian", fill = '#B8274C', alpha = 0.3) + labs(x = "Average monthly hours of employees who left") + xlim(min( min(HR.left$average_montly_hours), min(HR.stayed$average_montly_hours)), max( max(HR.left$average_montly_hours), max(HR.stayed$average_montly_hours)))
```

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.stayed, aes(HR.stayed$average_montly_hours)) + geom_density(kernel = "gaussian", fill = '#006E51', alpha = 0.3) + labs(x = "Average monthly hours of employees who stayed") + xlim(min( min(HR.left$average_montly_hours), min(HR.stayed$average_montly_hours)), max( max(HR.left$average_montly_hours), max(HR.stayed$average_montly_hours)))
```

### Time at company

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.left, aes(HR.left$time_spend_company)) + geom_histogram(fill = '#B8274C', binwidth = .5) + scale_x_continuous(breaks = seq(0,max(max(HR.left$time_spend_company),max(HR.stayed$time_spend_company)),by = 1), limits = c(0, max(max(HR.left$time_spend_company),max(HR.stayed$time_spend_company))+1)) + labs(x = "Time spent at the company (in years) of employees who left")
```

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.stayed, aes(HR.stayed$time_spend_company)) + geom_histogram(fill = '#006E51', binwidth = .5) + scale_x_continuous(breaks = seq(0,max(max(HR.left$time_spend_company),max(HR.stayed$time_spend_company)),by = 1), limits = c(0, max(max(HR.left$time_spend_company),max(HR.stayed$time_spend_company))+1)) + labs(x = "Time spent at the company (in years) of employees who stayed")
```

### Work accidents 

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.left, aes(HR.left$Work_accident)) + geom_histogram(fill = '#B8274C', binwidth = .5) + labs(x = "Number of employees who experienced and did not experience work accident of employees who left")
```

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.stayed, aes(HR.stayed$Work_accident)) + geom_histogram(fill = '#006E51', binwidth = .5) + labs(x = "Number of employees who experienced and did not experience work accident of employees who stayed")
```

### Promotions 

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.left, aes(HR.left$promotion_last_5years)) + geom_histogram(fill = '#B8274C', binwidth = .5) + labs(x = "Number of employees who were or were not promoted during last 5 years of employees who left")
```

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.stayed, aes(HR.stayed$promotion_last_5years)) + geom_histogram(fill = '#006E51', binwidth = .5) + labs(x = "Number of employees who were or were not promoted during last 5 years of employees who stayed")
```

### Department

```{r , echo=TRUE, message=FALSE, warning=FALSE}
Departments <- c("sales", "technical", "support", "IT", "product_mng", "marketing", "RandD", "accounting", "hr","management")
ggplot(HR.left, aes(HR.left$Division)) + geom_bar(stat = "count", fill = '#B8274C') + labs(x = "Department of employees who left") + scale_x_discrete(limits = Departments)
```

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.stayed, aes(HR.stayed$Division)) + geom_bar(stat = "count", fill = '#006E51') + labs(x = "Department of employees who stayed") + scale_x_discrete(limits = Departments)
```

### Salary

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.left, aes(HR.left$salary)) + geom_bar(stat = "count", fill = '#B8274C') + labs(x = "Salary level of employees who left") + scale_x_discrete(limits = c("low", "medium", "high"))
```

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ggplot(HR.stayed, aes(HR.stayed$salary)) + geom_bar(stat = "count", fill = '#006E51') + labs(x = "Salary level of employees who stayed") + scale_x_discrete(limits = c("low", "medium", "high"))
```

## Correlations

Let's check correlations between variables.

```{r}
CorTable <- cor(HR[,1:8])
corrplot(CorTable, method = "number", type= "upper")
```

`r if (max(CorTable [CorTable != 1]) < 0.7) { "Analysis shows that there are no strong correlation (0.7 or avobe) between any pair of numeric variables."} else "Analysis shows that strong correlation (0.7 or above) between numeric variables of the set is observed."`


# Data preparation

## Check for missing values

Let's count missing values in the dataset.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
sum(is.na(HR))
```

Since we don't have missing values, there is no need to exclude any observations from the dataset.

## Check for outliers

Graphical analysis of variables' distributions done on Data Understanding step did not show any suspicious outliers.

## Creation of dummy variables

Previously we identified, that two variables are categorical now: Department and Salary. Now we convert them to dummy variables, creating two dummy variables for Salary and nine dummy variables for Department.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
dummy <- dummyVars(" ~ .", data = HR, fullRank = TRUE)
HR.dm <- data.frame(predict(dummy, newdata = HR))
str(HR.dm)
```

We then re-checked for cross-correlations with the new dummy variables included. We see that none of the dummy variables are correlated with whether the employee left the firm (i.e all correlation < |0.7|). The only exception was between salary_medium and salary_low, which is expected because salary can only be medium or low. The result of this analysis do not support the necessity of removing variables.

```{r}
CorTable2 <- cor(HR.dm[,1:19])
corrplot(CorTable2, method = "color", addCoef.col="grey", type= "upper", number.cex=0.5)
```


## Range normalization

Classification tree methodology does not require scale normalization of input data, so we do not perform this step.

## Dimensionality reduction

Having small number of variables, we decided to avoid dimensionality reduction. 

## Correction of variables format

For using classification tree methodology, we convert part of variables to factor type.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
columns_to_factors <- c(6:19)
HR.dm[,columns_to_factors] <- lapply(HR.dm[,columns_to_factors], factor)
str(HR.dm)
```

## Separation into training and test datasets

Let's separate dataset into 80% training dataset and 20% test dataset.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
set.seed(777)
train.index <- createDataPartition(HR$left, p = .8, list = FALSE, times = 1)
TrainDS <- HR.dm[ train.index, ]
TestDS  <- HR.dm[-train.index, ]
```

There are `r nrow(TrainDS)` observations in training dataset and `r nrow(TestDS)` observations in test dataset.

We can check that both datasets are roughly similar in distribution of target variable (whether employee left).

```{r , echo=TRUE, message=FALSE, warning=FALSE}
round(prop.table(table(TrainDS$left)),3)
round(prop.table(table(TestDS$left)),3)
```

# Modelling: Decision Tree

## Choice of methodology and tool

Possible methodologies for solving classification problems include logistic regression, support vector machine, decision tree, Naive Bayes classification, k-nearest neighbor. For the purpose of this exercise we will use decision tree methodology. It should provide an output in a form of clear set of rules, that should be easy to communicate to management and easy to implement (for example, in company's HR system).

Our model should predict categorical response variable (if employee leaves) with two possible outcomes: yes (1) or no (0). For that reason, decision tree should be a classification tree. 

There are many packages in R for modeling decision trees: for example, rpart, party, RWeka, ipred, randomForest, gbm, C5.0. C5.0 is one of most up-to-date algorithms with following advantages ([source](http://en.proft.me/2016/11/9/classification-using-decision-trees-r/)):

+ An all-purpose classifier that does well on most problems.
+ Highly automatic learning process, which can handle numeric or nominal features, as well as missing data.
+ Less data cleaning required.
+ Excludes unimportant features.
+ Can be used on both small and large datasets.
+ Non parametric method (have no assumptions about the space distribution and the classifier structure).
+ Results in a model that can be interpreted without a mathematical background (for relatively small trees).
+ More efficient than other complex models.

C4.5, which was a C5.0 predecessor, became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008 ([source](https://en.wikipedia.org/wiki/C4.5_algorithm)). Taking all advantages into account, we are using C5.0 algorithm for the modelling.

C5.0 builds decision trees from a set of training data using the concept of information entropy. At each node of the tree, C5.0 chooses the attribute of the data that most effectively splits its set of samples into subsets enriched in one class or the other. The splitting criterion is the normalized information gain (difference in entropy). The attribute with the highest normalized information gain is chosen to make the decision. The C5.0 algorithm then recurs on the smaller sublists.

## First application of tool to dataset

For a first iteration, we apply C5.0 algorithm to the whole training dataset. Variable "Leave" serves as target variables and all other variables - as predictors. After the initial modelling we'll see which predicting variables are less helpful in predicting leave of employees and can be removed from modelling process.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ClTree1 <- C5.0(TrainDS[-7],TrainDS$left)
summary(ClTree1)
plot(ClTree1)
```

## Simplification of the model

As we see from results of the first iteration, only five variables are mainly used by the model for making sufficiently precise prediction: average_montly_hours; satisfaction_level;	time_spend_company;	last_evaluation; number_project.

We will build a model only with these variables and will later evaluate performance of this simplified model on test dataset.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
ClTree2 <- C5.0(TrainDS[c(4, 1, 5, 2, 3)],TrainDS$left)
summary(ClTree2)

```

# Evaluation

To evaluate the model we apply it to test dataset and build the confusion matrix of results. It shows how many mistakes of different type the model does and how many observations are classified correctly.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
PredictionTree <- predict(ClTree2, TestDS)
CT.Tree <- CrossTable(TestDS$left, PredictionTree, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE, dnn = c('actual leave', 'predicted leave'))
CT.Tree.DF <- as.data.frame(CT.Tree)
```

As we can see from the table, for `r CT.Tree.DF[4,3]+CT.Tree.DF[1,3]` out of `r nrow(TestDS)` observations in test dataset model correctly predicted if employee will leave. This includes `r CT.Tree.DF[1,3]` observations where model correctly predicted that employee will not leave and `r CT.Tree.DF[4,3]` observation where model correctly predicted leave. Thus `r round(CT.Tree.DF[4,12]+CT.Tree.DF[1,12],3)*100`% observations from test dataset were classified correctly.

Dataset was initially imbalanced (`r round(sum(HR$left==1)/nrow(HR),2)*100`% of all employees in dataset did not leave). Even that we already see significant information gain vs random gassing from conclusion above, let's check how successfully model identifies observations of both positive and negative target variable.

Model correctly identified `r round(CT.Tree.DF[4,3]/(CT.Tree.DF[4,3]+CT.Tree.DF[2,3]),3)*100`% of positive observations (the ones where employee left). Only `r CT.Tree.DF[2,3]` of employees who in fact left were not identified by the model.

Model also correctly identified `r round(CT.Tree.DF[1,3]/(CT.Tree.DF[1,3]+CT.Tree.DF[3,3]),3)*100`% of negative observations. Only for `r CT.Tree.DF[3,3]` of employees model generated false positive prediction.


# Deployment

As a result of the project, we built a model that proved to be a successful predictor of possible leave of employees. It turned out, that most powerful predictors of leave are working hours, satisfaction level, time spent at company, last evaluation, and number of projects. But this data should be used in a systematic logical way, described by decision tree we built - none of variables alone can predict a leave (this we saw from correlation analysis at the beginning). Furthermore, while the model was built with backward looking data, the prediction tool uses data on employees that are still working at the company during data collection. Prediction result must be interpreted carefully, as we will present below.

## Deployment Example 1: Early Indicator of Employee Leaving

Let's say we collected data on three current employees in a company:

```{r , echo=TRUE, message=FALSE, warning=FALSE}
data.deploy1 <- read.csv("./Data/Deploy_Data1.csv", sep=",", dec=".")
data.deploy1
```

We will apply the model we have built to Jackie's, Shahrukh's and Brad's data as below:

```{r , echo=TRUE, message=FALSE, warning=FALSE}
predict.deploy <- predict(ClTree2,data.deploy1)
ftable(data.deploy1$name,predict.deploy, dnn = c("Name","Prediction"))
```

While all three employees were still employed in the company, the model gave a 'leave' prediction to Jackie, and a 'stay' prediction to Brad and Shahrukh. HR should interpret this data the following way:

+ There is a high probability that Jackie will leave the company in the near future. HR can then decide whether they should take pre-emptive measures to prevent Jackie from leaving
+ There is a high probability that Brad and Shahrukh will stay in the company in the near future. HR does not need to take immediate action. However, their data must be reevaluated routinely as the data will change (for example, time spend in the company will continue to increase or employees get new projects)

## Deployment Example 2: HR Forecasting

In most cases, employee satisfaction surveys are confidential and HR does not have visibility to specific employee's data. For example, we have a data of a company with 10 employees:

```{r , echo=TRUE, message=FALSE, warning=FALSE}
data.deploy2 <- read.csv("./Data/Deploy_Data2.csv", sep=",", dec=".")
data.deploy2
```

We can then generate a report of a percentage of employees that has a high probability in leaving.

```{r , echo=TRUE, message=FALSE, warning=FALSE}
predict.deploy2 <- predict(ClTree2,data.deploy2)
ftable(predict.deploy2, dnn = c("Prediction"))
```
In this case, we see that 1 in 10 employees (or 10% of employees) have a high probability of leaving the company in the near future. Using this data, HR can set aside enough budget to plan for backfilling these employees.


# Further Improvements

Potential ways to improve this project include the following.

+ It's possible to build predictive models using other methodologies and tools, in order to find ways to improve predictive power even further.
+ It makes sense to integrate estimation of employee's value for the company in the model - so that company will try to stop from leaving only employees who generate enough positive results.
+ Tool can be further improved to advice HR manager what parameters of employee's work-life should be changes the first in order to decrease risk of leaving most significantly.